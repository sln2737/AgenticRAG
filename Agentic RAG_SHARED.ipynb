{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "65d592c6-18b4-4990-9325-759c67e55d37",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install faiss-cpu chromadb llama-cpp-python sentence-transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "87755481-8de8-4fda-bff6-5e5c41cd6ad6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import faiss\n",
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import json\n",
    "\n",
    "# Load the embedding model\n",
    "embedding_model = SentenceTransformer(\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "\n",
    "# Load text documents (replace with your proprietary data source)\n",
    "documents = [\n",
    "    \"Jamsetji Tata's vision laid the foundation for India's industrial revolution.\",\n",
    "    \"The Tata group has pioneered industries like steel, aviation, and IT.\",\n",
    "    \"Jamsetji's philosophy was about excellence, nation-building, and philanthropy.\",\n",
    "    \"The Tata Trusts have contributed significantly to education and healthcare.\",\n",
    "]\n",
    "\n",
    "# Generate embeddings\n",
    "embeddings = np.array(embedding_model.encode(documents), dtype=np.float32)\n",
    "\n",
    "# Create a FAISS index and add embeddings\n",
    "dimension = embeddings.shape[1]\n",
    "index = faiss.IndexFlatL2(dimension)\n",
    "index.add(embeddings)\n",
    "\n",
    "# Save FAISS index and document mapping\n",
    "faiss.write_index(index, \"vector_store.index\")\n",
    "\n",
    "# Save document mapping\n",
    "with open(\"doc_map.json\", \"w\") as f:\n",
    "    json.dump(documents, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2c352c18-9230-4694-8ed3-81b7f4cc257f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_relevant_documents(query, k=3):\n",
    "    \"\"\"Retrieve top-k relevant documents for a query using FAISS\"\"\"\n",
    "    query_embedding = np.array(embedding_model.encode([query]), dtype=np.float32)\n",
    "    distances, indices = index.search(query_embedding, k)\n",
    "\n",
    "    with open(\"doc_map.json\", \"r\") as f:\n",
    "        document_list = json.load(f)\n",
    "    \n",
    "    return [document_list[i] for i in indices[0]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0ef78df9-443e-4080-8232-db8b9586fbf3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".gitattributes\n",
      "README.md\n",
      "config.json\n",
      "deepseek-llm-7b-base.Q2_K.gguf\n",
      "deepseek-llm-7b-base.Q3_K_L.gguf\n",
      "deepseek-llm-7b-base.Q3_K_M.gguf\n",
      "deepseek-llm-7b-base.Q3_K_S.gguf\n",
      "deepseek-llm-7b-base.Q4_0.gguf\n",
      "deepseek-llm-7b-base.Q4_K_M.gguf\n",
      "deepseek-llm-7b-base.Q4_K_S.gguf\n",
      "deepseek-llm-7b-base.Q5_0.gguf\n",
      "deepseek-llm-7b-base.Q5_K_M.gguf\n",
      "deepseek-llm-7b-base.Q5_K_S.gguf\n",
      "deepseek-llm-7b-base.Q6_K.gguf\n",
      "deepseek-llm-7b-base.Q8_0.gguf\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9d5d02669504435190c9cc3a45ae7310",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "deepseek-llm-7b-base.Q8_0.gguf:  66%|######5   | 4.81G/7.35G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Could not set the permissions on the file 'C:\\models\\.cache\\huggingface\\download\\hKgVSwvTryGvDRigSBho6ExVV2g=.72da1d8c9050801d05fcc515c0d4706071591e6a52f03bb0eaff65ef78f6a50d.incomplete'. Error: [Errno 13] Permission denied: 'C:\\\\tmp_61b0a292-ee7b-4242-86ed-d5d1e5514b4b'.\n",
      "Continuing without setting permissions.\n"
     ]
    }
   ],
   "source": [
    "from huggingface_hub import hf_hub_download,HfApi\n",
    "import os\n",
    "\n",
    "# Security note: Never hardcode tokens! Use environment variables instead\n",
    "hf_token = os.getenv(\"HF_TOKEN\", \"ACCESS_TOKEN\")  # Replace with your actual token\n",
    "\n",
    "api = HfApi()\n",
    "files = api.list_repo_files(\n",
    "    repo_id=\"TheBloke/deepseek-llm-7B-base-GGUF\",\n",
    "    token=hf_token\n",
    ")\n",
    "\n",
    "for filename in files:\n",
    "    print(filename)\n",
    "\n",
    "model_path = hf_hub_download(\n",
    "    repo_id=\"TheBloke/deepseek-llm-7B-base-GGUF\",\n",
    "    filename=\"deepseek-llm-7b-base.Q8_0.gguf\",\n",
    "    token=hf_token,\n",
    "    local_dir=\"C:/models\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "befaa010-f83f-4540-b9fd-f252e29a4d7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_model_loader: loaded meta data with 23 key-value pairs and 273 tensors from C:/models/deepseek-llm-7b-base.Q8_0.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.name str              = LLaMA v2\n",
      "llama_model_loader: - kv   2:                       llama.context_length u32              = 4096\n",
      "llama_model_loader: - kv   3:                     llama.embedding_length u32              = 4096\n",
      "llama_model_loader: - kv   4:                          llama.block_count u32              = 30\n",
      "llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 11008\n",
      "llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 32\n",
      "llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000001\n",
      "llama_model_loader: - kv  10:                       llama.rope.freq_base f32              = 10000.000000\n",
      "llama_model_loader: - kv  11:                          general.file_type u32              = 7\n",
      "llama_model_loader: - kv  12:                       tokenizer.ggml.model str              = gpt2\n",
      "llama_model_loader: - kv  13:                      tokenizer.ggml.tokens arr[str,102400]  = [\"!\", \"\\\"\", \"#\", \"$\", \"%\", \"&\", \"'\", ...\n",
      "llama_model_loader: - kv  14:                      tokenizer.ggml.scores arr[f32,102400]  = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  15:                  tokenizer.ggml.token_type arr[i32,102400]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\n",
      "llama_model_loader: - kv  16:                      tokenizer.ggml.merges arr[str,99757]   = [\"Ġ Ġ\", \"Ġ t\", \"Ġ a\", \"i n\", \"h e...\n",
      "llama_model_loader: - kv  17:                tokenizer.ggml.bos_token_id u32              = 10000\n",
      "llama_model_loader: - kv  18:                tokenizer.ggml.eos_token_id u32              = 10001\n",
      "llama_model_loader: - kv  19:            tokenizer.ggml.padding_token_id u32              = 10001\n",
      "llama_model_loader: - kv  20:               tokenizer.ggml.add_bos_token bool             = true\n",
      "llama_model_loader: - kv  21:               tokenizer.ggml.add_eos_token bool             = false\n",
      "llama_model_loader: - kv  22:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - type  f32:   61 tensors\n",
      "llama_model_loader: - type q8_0:  212 tensors\n",
      "print_info: file format = GGUF V3 (latest)\n",
      "print_info: file type   = Q8_0\n",
      "print_info: file size   = 6.84 GiB (8.50 BPW) \n",
      "load: missing pre-tokenizer type, using: 'default'\n",
      "load:                                             \n",
      "load: ************************************        \n",
      "load: GENERATION QUALITY WILL BE DEGRADED!        \n",
      "load: CONSIDER REGENERATING THE MODEL             \n",
      "load: ************************************        \n",
      "load:                                             \n",
      "init_tokenizer: initializing tokenizer for type 2\n",
      "load: control token: 100001 '<｜end▁of▁sentence｜>' is not marked as EOG\n",
      "load: control token: 100000 '<｜begin▁of▁sentence｜>' is not marked as EOG\n",
      "load: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect\n",
      "load: special_eot_id is not in special_eog_ids - the tokenizer config may be incorrect\n",
      "load: special tokens cache size = 2400\n",
      "load: token to piece cache size = 0.6681 MB\n",
      "print_info: arch             = llama\n",
      "print_info: vocab_only       = 0\n",
      "print_info: n_ctx_train      = 4096\n",
      "print_info: n_embd           = 4096\n",
      "print_info: n_layer          = 30\n",
      "print_info: n_head           = 32\n",
      "print_info: n_head_kv        = 32\n",
      "print_info: n_rot            = 128\n",
      "print_info: n_swa            = 0\n",
      "print_info: n_embd_head_k    = 128\n",
      "print_info: n_embd_head_v    = 128\n",
      "print_info: n_gqa            = 1\n",
      "print_info: n_embd_k_gqa     = 4096\n",
      "print_info: n_embd_v_gqa     = 4096\n",
      "print_info: f_norm_eps       = 0.0e+00\n",
      "print_info: f_norm_rms_eps   = 1.0e-06\n",
      "print_info: f_clamp_kqv      = 0.0e+00\n",
      "print_info: f_max_alibi_bias = 0.0e+00\n",
      "print_info: f_logit_scale    = 0.0e+00\n",
      "print_info: n_ff             = 11008\n",
      "print_info: n_expert         = 0\n",
      "print_info: n_expert_used    = 0\n",
      "print_info: causal attn      = 1\n",
      "print_info: pooling type     = 0\n",
      "print_info: rope type        = 0\n",
      "print_info: rope scaling     = linear\n",
      "print_info: freq_base_train  = 10000.0\n",
      "print_info: freq_scale_train = 1\n",
      "print_info: n_ctx_orig_yarn  = 4096\n",
      "print_info: rope_finetuned   = unknown\n",
      "print_info: ssm_d_conv       = 0\n",
      "print_info: ssm_d_inner      = 0\n",
      "print_info: ssm_d_state      = 0\n",
      "print_info: ssm_dt_rank      = 0\n",
      "print_info: ssm_dt_b_c_rms   = 0\n",
      "print_info: model type       = ?B\n",
      "print_info: model params     = 6.91 B\n",
      "print_info: general.name     = LLaMA v2\n",
      "print_info: vocab type       = BPE\n",
      "print_info: n_vocab          = 102400\n",
      "print_info: n_merges         = 99757\n",
      "print_info: BOS token        = 10000 'ĠÐ³Ð¾Ð´Ð¸Ð½Ð¸'\n",
      "print_info: EOS token        = 10001 'ĠThomas'\n",
      "print_info: EOT token        = 100001 '<｜end▁of▁sentence｜>'\n",
      "print_info: PAD token        = 10001 'ĠThomas'\n",
      "print_info: LF token         = 126 'Ä'\n",
      "print_info: EOG token        = 10001 'ĠThomas'\n",
      "print_info: EOG token        = 100001 '<｜end▁of▁sentence｜>'\n",
      "print_info: max token length = 256\n",
      "load_tensors: layer   0 assigned to device CPU\n",
      "load_tensors: layer   1 assigned to device CPU\n",
      "load_tensors: layer   2 assigned to device CPU\n",
      "load_tensors: layer   3 assigned to device CPU\n",
      "load_tensors: layer   4 assigned to device CPU\n",
      "load_tensors: layer   5 assigned to device CPU\n",
      "load_tensors: layer   6 assigned to device CPU\n",
      "load_tensors: layer   7 assigned to device CPU\n",
      "load_tensors: layer   8 assigned to device CPU\n",
      "load_tensors: layer   9 assigned to device CPU\n",
      "load_tensors: layer  10 assigned to device CPU\n",
      "load_tensors: layer  11 assigned to device CPU\n",
      "load_tensors: layer  12 assigned to device CPU\n",
      "load_tensors: layer  13 assigned to device CPU\n",
      "load_tensors: layer  14 assigned to device CPU\n",
      "load_tensors: layer  15 assigned to device CPU\n",
      "load_tensors: layer  16 assigned to device CPU\n",
      "load_tensors: layer  17 assigned to device CPU\n",
      "load_tensors: layer  18 assigned to device CPU\n",
      "load_tensors: layer  19 assigned to device CPU\n",
      "load_tensors: layer  20 assigned to device CPU\n",
      "load_tensors: layer  21 assigned to device CPU\n",
      "load_tensors: layer  22 assigned to device CPU\n",
      "load_tensors: layer  23 assigned to device CPU\n",
      "load_tensors: layer  24 assigned to device CPU\n",
      "load_tensors: layer  25 assigned to device CPU\n",
      "load_tensors: layer  26 assigned to device CPU\n",
      "load_tensors: layer  27 assigned to device CPU\n",
      "load_tensors: layer  28 assigned to device CPU\n",
      "load_tensors: layer  29 assigned to device CPU\n",
      "load_tensors: layer  30 assigned to device CPU\n",
      "load_tensors: tensor 'token_embd.weight' (q8_0) (and 272 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead\n",
      "load_tensors:   CPU_Mapped model buffer size =  7002.83 MiB\n",
      "llama_init_from_model: n_seq_max     = 1\n",
      "llama_init_from_model: n_ctx         = 512\n",
      "llama_init_from_model: n_ctx_per_seq = 512\n",
      "llama_init_from_model: n_batch       = 512\n",
      "llama_init_from_model: n_ubatch      = 512\n",
      "llama_init_from_model: flash_attn    = 0\n",
      "llama_init_from_model: freq_base     = 10000.0\n",
      "llama_init_from_model: freq_scale    = 1\n",
      "llama_init_from_model: n_ctx_per_seq (512) < n_ctx_train (4096) -- the full capacity of the model will not be utilized\n",
      "llama_kv_cache_init: kv_size = 512, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 30, can_shift = 1\n",
      "llama_kv_cache_init: layer 0: n_embd_k_gqa = 4096, n_embd_v_gqa = 4096\n",
      "llama_kv_cache_init: layer 1: n_embd_k_gqa = 4096, n_embd_v_gqa = 4096\n",
      "llama_kv_cache_init: layer 2: n_embd_k_gqa = 4096, n_embd_v_gqa = 4096\n",
      "llama_kv_cache_init: layer 3: n_embd_k_gqa = 4096, n_embd_v_gqa = 4096\n",
      "llama_kv_cache_init: layer 4: n_embd_k_gqa = 4096, n_embd_v_gqa = 4096\n",
      "llama_kv_cache_init: layer 5: n_embd_k_gqa = 4096, n_embd_v_gqa = 4096\n",
      "llama_kv_cache_init: layer 6: n_embd_k_gqa = 4096, n_embd_v_gqa = 4096\n",
      "llama_kv_cache_init: layer 7: n_embd_k_gqa = 4096, n_embd_v_gqa = 4096\n",
      "llama_kv_cache_init: layer 8: n_embd_k_gqa = 4096, n_embd_v_gqa = 4096\n",
      "llama_kv_cache_init: layer 9: n_embd_k_gqa = 4096, n_embd_v_gqa = 4096\n",
      "llama_kv_cache_init: layer 10: n_embd_k_gqa = 4096, n_embd_v_gqa = 4096\n",
      "llama_kv_cache_init: layer 11: n_embd_k_gqa = 4096, n_embd_v_gqa = 4096\n",
      "llama_kv_cache_init: layer 12: n_embd_k_gqa = 4096, n_embd_v_gqa = 4096\n",
      "llama_kv_cache_init: layer 13: n_embd_k_gqa = 4096, n_embd_v_gqa = 4096\n",
      "llama_kv_cache_init: layer 14: n_embd_k_gqa = 4096, n_embd_v_gqa = 4096\n",
      "llama_kv_cache_init: layer 15: n_embd_k_gqa = 4096, n_embd_v_gqa = 4096\n",
      "llama_kv_cache_init: layer 16: n_embd_k_gqa = 4096, n_embd_v_gqa = 4096\n",
      "llama_kv_cache_init: layer 17: n_embd_k_gqa = 4096, n_embd_v_gqa = 4096\n",
      "llama_kv_cache_init: layer 18: n_embd_k_gqa = 4096, n_embd_v_gqa = 4096\n",
      "llama_kv_cache_init: layer 19: n_embd_k_gqa = 4096, n_embd_v_gqa = 4096\n",
      "llama_kv_cache_init: layer 20: n_embd_k_gqa = 4096, n_embd_v_gqa = 4096\n",
      "llama_kv_cache_init: layer 21: n_embd_k_gqa = 4096, n_embd_v_gqa = 4096\n",
      "llama_kv_cache_init: layer 22: n_embd_k_gqa = 4096, n_embd_v_gqa = 4096\n",
      "llama_kv_cache_init: layer 23: n_embd_k_gqa = 4096, n_embd_v_gqa = 4096\n",
      "llama_kv_cache_init: layer 24: n_embd_k_gqa = 4096, n_embd_v_gqa = 4096\n",
      "llama_kv_cache_init: layer 25: n_embd_k_gqa = 4096, n_embd_v_gqa = 4096\n",
      "llama_kv_cache_init: layer 26: n_embd_k_gqa = 4096, n_embd_v_gqa = 4096\n",
      "llama_kv_cache_init: layer 27: n_embd_k_gqa = 4096, n_embd_v_gqa = 4096\n",
      "llama_kv_cache_init: layer 28: n_embd_k_gqa = 4096, n_embd_v_gqa = 4096\n",
      "llama_kv_cache_init: layer 29: n_embd_k_gqa = 4096, n_embd_v_gqa = 4096\n",
      "llama_kv_cache_init:        CPU KV buffer size =   240.00 MiB\n",
      "llama_init_from_model: KV self size  =  240.00 MiB, K (f16):  120.00 MiB, V (f16):  120.00 MiB\n",
      "llama_init_from_model:        CPU  output buffer size =     0.39 MiB\n",
      "llama_init_from_model:        CPU compute buffer size =   208.00 MiB\n",
      "llama_init_from_model: graph nodes  = 966\n",
      "llama_init_from_model: graph splits = 1\n",
      "CPU : SSE3 = 1 | SSSE3 = 1 | AVX = 1 | AVX2 = 1 | F16C = 1 | FMA = 1 | AVX512 = 1 | LLAMAFILE = 1 | OPENMP = 1 | AARCH64_REPACK = 1 | \n",
      "Model metadata: {'general.name': 'LLaMA v2', 'general.architecture': 'llama', 'llama.context_length': '4096', 'llama.rope.dimension_count': '128', 'llama.embedding_length': '4096', 'llama.block_count': '30', 'llama.feed_forward_length': '11008', 'llama.attention.head_count': '32', 'tokenizer.ggml.eos_token_id': '10001', 'general.file_type': '7', 'llama.attention.head_count_kv': '32', 'llama.attention.layer_norm_rms_epsilon': '0.000001', 'llama.rope.freq_base': '10000.000000', 'tokenizer.ggml.model': 'gpt2', 'general.quantization_version': '2', 'tokenizer.ggml.bos_token_id': '10000', 'tokenizer.ggml.padding_token_id': '10001', 'tokenizer.ggml.add_bos_token': 'true', 'tokenizer.ggml.add_eos_token': 'false'}\n",
      "Using fallback chat format: llama-2\n",
      "llama_perf_context_print:        load time =    7975.94 ms\n",
      "llama_perf_context_print: prompt eval time =    7975.21 ms /   107 tokens (   74.53 ms per token,    13.42 tokens per second)\n",
      "llama_perf_context_print:        eval time =   66509.04 ms /   299 runs   (  222.44 ms per token,     4.50 tokens per second)\n",
      "llama_perf_context_print:       total time =   75216.52 ms /   406 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Jamsetji Tata's vision laid the foundation for India's industrial revolution.\n",
      "    2. The Tata group has pioneered industries like steel, aviation, and IT.\n",
      "    3. Jamsetji's philosophy was about excellence, nation-building, and philanthropy.\n",
      "\n",
      "    Hint: You can use the following code blocks to generate a document.\n",
      "\n",
      "    ```python\n",
      "    # Define a template for the document\n",
      "    template = (\n",
      "        \"Jamsetji Tata's vision laid the foundation for India's industrial \"\n",
      "        \"revolution. The Tata group has pioneered industries like steel, \"\n",
      "        \"aviation, and IT. Jamsetji's philosophy was about excellence, \"\n",
      "        \"nation-building, and philanthropy.\"\n",
      "    )\n",
      "    \n",
      "    # Generate the document using the template and the given retrieved documents\n",
      "    document = (\n",
      "        f\"You are an AI agent using Retrieval-Augmented Generation (RAG). \"\n",
      "        f\"Answer the query using the following retrieved documents:\"\n",
      "    ) + template + (\n",
      "        \"\\n\\nQuery: What was Jamsetji Tata's industrial impact?\\nAnswer:\"\n",
      "    ) + \"\\n\" + \"\\n\".join(retrieved_docs)\n",
      "    ```\n",
      "\n",
      "    Hint 2: You can use the following code blocks to add a new document to the list of retrieved documents.\n",
      "\n",
      "    ```python\n"
     ]
    }
   ],
   "source": [
    "from llama_cpp import Llama\n",
    "\n",
    "# Load the on-prem model (adjust the path)\n",
    "llm = Llama(model_path=\"C:/models/deepseek-llm-7b-base.Q8_0.gguf\")\n",
    "\n",
    "def generate_response(query):\n",
    "    \"\"\"Generate a response using retrieved context and the LLM\"\"\"\n",
    "    retrieved_docs = retrieve_relevant_documents(query)\n",
    "    context = \"\\n\".join(retrieved_docs)\n",
    "    \n",
    "    prompt = f\"\"\"You are an AI agent using Retrieval-Augmented Generation (RAG). \n",
    "    Answer the query using the following retrieved documents:\n",
    "\n",
    "    {context}\n",
    "\n",
    "    Query: {query}\n",
    "    Answer:\n",
    "    \"\"\"\n",
    "\n",
    "    response = llm(prompt, max_tokens=300)\n",
    "    return response[\"choices\"][0][\"text\"]\n",
    "\n",
    "# Example query\n",
    "query = \"What was Jamsetji Tata's industrial impact?\"\n",
    "response = generate_response(query)\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "83749a41-e21a-4f3c-b9da-da8dd2ca0338",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 1 prefix-match hit, remaining 47 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    7975.94 ms\n",
      "llama_perf_context_print: prompt eval time =    3729.47 ms /    47 tokens (   79.35 ms per token,    12.60 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2021.41 ms /     9 runs   (  224.60 ms per token,     4.45 tokens per second)\n",
      "llama_perf_context_print:       total time =    5771.57 ms /    56 tokens\n",
      "Llama.generate: 1 prefix-match hit, remaining 5 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    7975.94 ms\n",
      "llama_perf_context_print: prompt eval time =     534.38 ms /     5 tokens (  106.88 ms per token,     9.36 tokens per second)\n",
      "llama_perf_context_print:        eval time =   68075.19 ms /   299 runs   (  227.68 ms per token,     4.39 tokens per second)\n",
      "llama_perf_context_print:       total time =   69373.74 ms /   304 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "What is Tata Steel in the UK?\n",
      "Tata Steel Europe, based in the UK, is one of the world’s leading steel producers with 38,000 employees, 270 locations and a turnover of around £10 billion.\n",
      "How many employees does Tata Steel UK have?\n",
      "The company operates in the UK, France, Italy, the Netherlands, Germany and Belgium. In 2012, Tata Steel had 29,000 employees.\n",
      "When was the Tata Group founded?\n",
      "Tata Group, incorporated in 19th June 1868 as a private limited company, is an Indian conglomerate.\n",
      "When was Tata Steel established?\n",
      "In 1907, Tata Steel, then known as The Tata Iron and Steel Company, was established. In 1932, the company became the world’s second-largest steel producer, after US Steel.\n",
      "Who is the largest steel producer in the world?\n",
      "Steel is one of the most important materials in the world. The world’s largest steel-producing company is ArcelorMittal, with 221.7 million tons in 2018. The largest steel-producing country is China with 1.058 billion tons.\n",
      "What is the biggest Tata Group company?\n",
      "Tata Consultancy Services, a subsidiary of Tata Sons, is one of the world’s largest IT\n"
     ]
    }
   ],
   "source": [
    "class Agent:\n",
    "    \"\"\"Custom agent to decide whether to retrieve, generate, or refine responses\"\"\"\n",
    "\n",
    "    def __init__(self, llm):\n",
    "        self.llm = llm\n",
    "\n",
    "    def decide_action(self, query):\n",
    "        \"\"\"Decide if retrieval is necessary or if LLM alone can answer\"\"\"\n",
    "        prompt = f\"\"\"Determine if the query requires external retrieval. \n",
    "        Respond with 'retrieve' if knowledge from documents is needed, otherwise 'generate':\n",
    "\n",
    "        Query: {query}\n",
    "        Answer:\n",
    "        \"\"\"\n",
    "        response = self.llm(prompt, max_tokens=10)[\"choices\"][0][\"text\"].strip().lower()\n",
    "        return response\n",
    "\n",
    "    def execute(self, query):\n",
    "        \"\"\"Execute the best approach based on decision\"\"\"\n",
    "        action = self.decide_action(query)\n",
    "\n",
    "        if \"retrieve\" in action:\n",
    "            return generate_response(query)\n",
    "        else:\n",
    "            return self.llm(query, max_tokens=300)[\"choices\"][0][\"text\"]\n",
    "\n",
    "# Initialize agent\n",
    "agent = Agent(llm)\n",
    "\n",
    "# Example agent decision\n",
    "query = \"Who founded Tata Steel?\"\n",
    "response = agent.execute(query)\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "35ae9ca9-f5fb-405c-aa09-3f6ab5997ad0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 1 prefix-match hit, remaining 50 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    7975.94 ms\n",
      "llama_perf_context_print: prompt eval time =    2698.97 ms /    50 tokens (   53.98 ms per token,    18.53 tokens per second)\n",
      "llama_perf_context_print:        eval time =    1792.93 ms /     9 runs   (  199.21 ms per token,     5.02 tokens per second)\n",
      "llama_perf_context_print:       total time =    4508.27 ms /    59 tokens\n",
      "Llama.generate: 1 prefix-match hit, remaining 8 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    7975.94 ms\n",
      "llama_perf_context_print: prompt eval time =     483.43 ms /     8 tokens (   60.43 ms per token,    16.55 tokens per second)\n",
      "llama_perf_context_print:        eval time =   65574.39 ms /   299 runs   (  219.31 ms per token,     4.56 tokens per second)\n",
      "llama_perf_context_print:       total time =   66780.06 ms /   307 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Who is the Owner of Tata Steel?\n",
      "The Tata Group is a large conglomerate that is owned by the Tata family. The Tata Group is involved in a number of different businesses, including steel, hospitality, automobiles, and telecommunications. The Tata Group is one of the largest and most successful conglomerates in the world.\n",
      "How to Contact Tata Steel?\n",
      "If you have any questions about Tata Steel or if you would like to contact us for any reason, please feel free to do so. You can reach us at the following address:\n",
      "Tata Steel Limited\n",
      "1777 West Loop South\n",
      "Suite 600\n",
      "Houston, TX 77027\n",
      "Phone: 713.626.4241\n",
      "Fax: 713.626.4222\n",
      "We will be happy to assist you in any way we can.\n",
      "Tata Steel has a long and storied history in the steel industry. The company has been around for over 150 years and is one of the largest steel producers in the world. Tata Steel is a part of the Tata Group, which is one of the largest conglomerates in India. The company has operations in over 50 countries around the world and employs over 300,000 people.\n",
      "Tata Steel is headquartered in Mumbai, India and has a number of steel plants located around the world. The company produces a wide range\n"
     ]
    }
   ],
   "source": [
    "# Example agent decision\n",
    "query = \"Who is the Owner of Tata Steel?\"\n",
    "response = agent.execute(query)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99167e86-f1ab-4c2f-8a55-ab56adf8c2bd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {
     "0582cad15c614dba9cd40a8997cc5b77": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "bar_style": "danger",
       "layout": "IPY_MODEL_1168a8ebe6d24838bdb1ab5b19780c6e",
       "max": 7347395872,
       "style": "IPY_MODEL_251843e01cf64e0fafa09f8c42e269ed",
       "value": 4812963840
      }
     },
     "1168a8ebe6d24838bdb1ab5b19780c6e": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "1ab5b367c9524eff840572851de9c6ae": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "layout": "IPY_MODEL_3e4548b3a1ae4f629cd55bf8794097c5",
       "style": "IPY_MODEL_34b7ca5152e9401f9531c4fb0c5f7602",
       "value": "deepseek-llm-7b-base.Q8_0.gguf: 100%"
      }
     },
     "251843e01cf64e0fafa09f8c42e269ed": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "description_width": ""
      }
     },
     "34b7ca5152e9401f9531c4fb0c5f7602": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "3e4548b3a1ae4f629cd55bf8794097c5": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "4be8a0207760457689010bdab8b73b54": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "layout": "IPY_MODEL_ec8eef33c9e94365858c4b211175045f",
       "style": "IPY_MODEL_e0363bb27b8844378c4bbc0cbf87d1b9",
       "value": " 7.35G/7.35G [03:42&lt;00:00, 11.7MB/s]"
      }
     },
     "53dfef184b1c464d9abb093b0160265a": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "6ea866a08e3e4e1fbe4e18272e9836a7": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "bar_style": "success",
       "layout": "IPY_MODEL_ac683189f6674a508862089187083d6f",
       "max": 7347395872,
       "style": "IPY_MODEL_92db00b6fe5745239f816cccc11e17ec",
       "value": 7347395872
      }
     },
     "92db00b6fe5745239f816cccc11e17ec": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "description_width": ""
      }
     },
     "99838103d78d4b2bace9a1f20a8a86e2": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "children": [
        "IPY_MODEL_b4a86d14f17348a48c4c0d0acc441b31",
        "IPY_MODEL_0582cad15c614dba9cd40a8997cc5b77",
        "IPY_MODEL_f06abf088b7d40128c9dc4dd73691291"
       ],
       "layout": "IPY_MODEL_fdca9b7612ad4c92b89c8ae9cf80cdfc"
      }
     },
     "9d5d02669504435190c9cc3a45ae7310": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "children": [
        "IPY_MODEL_1ab5b367c9524eff840572851de9c6ae",
        "IPY_MODEL_6ea866a08e3e4e1fbe4e18272e9836a7",
        "IPY_MODEL_4be8a0207760457689010bdab8b73b54"
       ],
       "layout": "IPY_MODEL_53dfef184b1c464d9abb093b0160265a"
      }
     },
     "a439cb6fc0294ec5afdc8e0ee8688188": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "ac683189f6674a508862089187083d6f": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "b4a86d14f17348a48c4c0d0acc441b31": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "layout": "IPY_MODEL_d1df2b0caae542e58962ce108809b543",
       "style": "IPY_MODEL_fc6fa245c81b47e4863d983f45ff511b",
       "value": "deepseek-llm-7b-base.Q8_0.gguf:  66%"
      }
     },
     "bb84bcd98be94b438b926fa61f77e1a0": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "d1df2b0caae542e58962ce108809b543": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "e0363bb27b8844378c4bbc0cbf87d1b9": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "ec8eef33c9e94365858c4b211175045f": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "f06abf088b7d40128c9dc4dd73691291": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "layout": "IPY_MODEL_a439cb6fc0294ec5afdc8e0ee8688188",
       "style": "IPY_MODEL_bb84bcd98be94b438b926fa61f77e1a0",
       "value": " 4.81G/7.35G [10:14&lt;03:35, 11.8MB/s]"
      }
     },
     "fc6fa245c81b47e4863d983f45ff511b": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "fdca9b7612ad4c92b89c8ae9cf80cdfc": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     }
    },
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
